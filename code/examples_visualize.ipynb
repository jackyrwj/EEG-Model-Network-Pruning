{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.7'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torcheeg\n",
    "\n",
    "torcheeg.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: ['tmp_in', 'tmp_out', 'test', 'CONTRIBUTING.md', 'dist', '.git', 'LICENSE', 'build', '.readthedocs.yaml', 'setup.py', 'MANIFEST.in', 'docs', 'README.md', '.eggs', 'examples', 'torcheeg.egg-info', '.vscode', 'torcheeg', '.gitignore', 'README.rst']\n",
      "Data Directory: ['data_preprocessed_python', 'Sessions', 'data_preprocessed', 'DREAMER.mat', 'Preprocessed_EEG']\n",
      "IO Directory: ['deap_aniertcjwovsqklfbxhz', 'deap_pxiqzscnaofdvgujwrtl', 'seed_zcwanuphedgsvormtbkf', 'deap_wvzkxcomunqrlbsapfie', 'deap_gkieldhfxmyzjwavcpuo', 'deap_ovbahuwmxrsictydqnef']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print('Current Directory:', os.listdir('../'))\n",
    "print('Data Directory:', os.listdir('../tmp_in'))\n",
    "print('IO Directory:', os.listdir('../tmp_out'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangzhi/anaconda3/envs/pytorch-geometric/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[DEAP]: 100%|██████████| 32/32 [00:12<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait for the writing process to complete...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from torcheeg import transforms\n",
    "from torcheeg.datasets import DEAPDataset\n",
    "\n",
    "dataset = DEAPDataset(io_path=f'../tmp_out/deap_{\"\".join(random.sample(\"zyxwvutsrqponmlkjihgfedcba\", 20))}',\n",
    "                      root_path='../tmp_in/data_preprocessed_python',\n",
    "                      online_transform=transforms.Compose([transforms.To2d(), transforms.ToTensor()]),\n",
    "                      label_transform=transforms.Compose([\n",
    "                          transforms.Select('valence'),\n",
    "                          transforms.Binary(5.0),\n",
    "                      ]),\n",
    "                      num_worker=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cell to prevent misuse of the subject's real EEG signals\n",
    "\n",
    "from torcheeg.datasets.constants.emotion_recognition.deap import DEAP_CHANNEL_LIST\n",
    "from torcheeg.utils import plot_raw_topomap\n",
    "\n",
    "img = plot_raw_topomap(dataset[0][0].squeeze(), channel_list=DEAP_CHANNEL_LIST, sampling_rate=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cell to prevent misuse of the subject's real EEG signals\n",
    "\n",
    "from torcheeg.utils import plot_signal\n",
    "\n",
    "img = plot_signal(dataset[0][0].squeeze(), channel_list=DEAP_CHANNEL_LIST, sampling_rate=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cell to prevent misuse of the subject's real EEG signals\n",
    "\n",
    "from torcheeg.utils import plot_2d_tensor\n",
    "\n",
    "img = plot_2d_tensor(dataset[0][0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SEED]: 100%|██████████| 45/45 [29:01<00:00, 38.70s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait for the writing process to complete...\n"
     ]
    }
   ],
   "source": [
    "from torcheeg.datasets import SEEDDataset\n",
    "from torcheeg.datasets.constants.emotion_recognition.seed import SEED_ADJACENCY_MATRIX\n",
    "\n",
    "dataset = SEEDDataset(io_path=f'../tmp_out/seed_{\"\".join(random.sample(\"zyxwvutsrqponmlkjihgfedcba\", 20))}',\n",
    "                      root_path='../tmp_in/Preprocessed_EEG',\n",
    "                      offline_transform=transforms.BandDifferentialEntropy(),\n",
    "                      online_transform=transforms.ToG(SEED_ADJACENCY_MATRIX),\n",
    "                      label_transform=transforms.Compose([\n",
    "                          transforms.Select('emotion'),\n",
    "                          transforms.Lambda(lambda x: int(x) + 1),\n",
    "                      ]),\n",
    "                      num_worker=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cell to prevent misuse of the subject's real EEG signals\n",
    "\n",
    "from torcheeg.utils import plot_graph\n",
    "from torcheeg.datasets.constants.emotion_recognition.seed import SEED_CHANNEL_LOCATION_DICT\n",
    "\n",
    "img = plot_graph(dataset[0][0], SEED_CHANNEL_LOCATION_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[AMIGOS]: 100%|██████████| 40/40 [02:06<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait for the writing process to complete...\n"
     ]
    }
   ],
   "source": [
    "from torcheeg.datasets import AMIGOSDataset\n",
    "from torcheeg.datasets.constants.emotion_recognition.amigos import AMIGOS_CHANNEL_LOCATION_DICT\n",
    "\n",
    "dataset = AMIGOSDataset(io_path=f'../tmp_out/amigos_{\"\".join(random.sample(\"zyxwvutsrqponmlkjihgfedcba\", 20))}',\n",
    "                        root_path='../tmp_in/data_preprocessed',\n",
    "                        trial_num=16,\n",
    "                        offline_transform=transforms.Compose(\n",
    "                            [transforms.BandDifferentialEntropy(),\n",
    "                             transforms.ToGrid(AMIGOS_CHANNEL_LOCATION_DICT)]),\n",
    "                        online_transform=transforms.ToTensor(),\n",
    "                        label_transform=transforms.Compose([\n",
    "                            transforms.Select('valence'),\n",
    "                            transforms.Binary(5.0),\n",
    "                        ]),\n",
    "                        num_worker=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cell to prevent misuse of the subject's real EEG signals\n",
    "\n",
    "from torcheeg.utils import plot_3d_tensor\n",
    "\n",
    "img = plot_3d_tensor(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torcheeg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "88fd0678318cef621c62179797f73f2ae7e1fb8b9d4460636f23e7774c6ecc95"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
